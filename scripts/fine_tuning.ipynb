{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dict = {'MRPC': {'path': \"SetFit/mrpc\", 'data': ('text1', 'text2')}, 'QQP': {'path': \"SetFit/qqp\", 'data': ('text1', 'text2')}, \\\n",
    "            'SST': {'path': \"sst\", 'data': ('sentence')}, 'ANLI': {'path': \"facebook/anli\", 'data': ('premise', 'hypothesis')}}\n",
    "\n",
    "models_dict = {'BERT-Base': \"google-bert/bert-base-uncased\", 'BERT-Large': \"google-bert/bert-large-uncased\", \\\n",
    "          'RoBERTa-Base': \"FacebookAI/roberta-base\", 'RoBERTa-Large': \"FacebookAI/roberta-large\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTune():\n",
    "    def __init__(self, model_path, dataset_dict):\n",
    "        # Load the dataset\n",
    "        print(f\"Loading the dataset from {dataset_dict['path']}\")\n",
    "        self.dataset = load_dataset(dataset_dict['path'], split = 'train')\n",
    "        self.data = dataset_dict['data']\n",
    "        # Task is paraphrasing if we have more than one data column\n",
    "        self.paraphrase = (len(self.data) == 1)\n",
    "\n",
    "        print('Generating tokens for the dataset')\n",
    "        self.tokenizer =  AutoTokenizer.from_pretrained(model_path)\n",
    "        self.tokenized_dataset = self.tokenize()\n",
    "        self.num_labels = len(set(self.dataset['label']))\n",
    "\n",
    "        print(f'Loading the model from {model_path}')\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=self.num_labels)\n",
    "\n",
    "    def tokenize(self):\n",
    "        if(self.paraphrase):\n",
    "            prompt = \"{text1}\\nPARAPHRASE:\\n{text2}\"\n",
    "            col1, col2 = self.data\n",
    "            text_data = [prompt.format(text1=sentence1, text2=sentence2) for sentence1, sentence2 in zip(self.dataset[col1], self.dataset[col2])]\n",
    "            self.dataset = self.dataset.add_column('text', text_data)\n",
    "            self.data = 'text'\n",
    "        else:\n",
    "            self.data = self.data[0]\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            return self.tokenizer(examples[self.data], padding=\"max_length\", truncation=True)\n",
    "\n",
    "        tokenized_dataset = self.dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "        return tokenized_dataset\n",
    "\n",
    "    def train(self, num_epochs=6):\n",
    "        training_args = TrainingArguments(output_dir=\"test_trainer\", num_train_epochs=num_epochs)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.tokenized_dataset\n",
    "        )\n",
    "\n",
    "        trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "from peft import get_peft_model\n",
    "\n",
    "class PEFT(FineTune):\n",
    "    def __init__(self, model_path, dataset_dict, lora_rank=4):\n",
    "        super().__init__(model_path, dataset_dict)\n",
    "        self.rank = lora_rank\n",
    "        self.config = LoraConfig(task_type=TaskType.SEQ_CLS, inference_mode=False, r=self.rank, lora_alpha=32)\n",
    "        self.model = get_peft_model(self.model, self.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset from SetFit/mrpc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsanten/anaconda3/lib/python3.11/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating tokens for the dataset\n",
      "Loading the model from google-bert/bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = FineTune(models_dict['BERT-Base'], datasets_dict['MRPC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
